{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "#!pip install datawig\n",
    "#import datawig\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from pycaret.classification import *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#!pip install pytorch_tabnet\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model  import TabNetClassifier\n",
    "\n",
    "#!pip install optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n",
    "import time\n",
    "\n",
    "#!pip install tensorflow_addons == \"0.22.0\"\n",
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"complete_train_duplication.csv\")\n",
    "submission = pd.read_csv(\"complete_submission_duplication.csv\")\n",
    "\n",
    "converted_mean = sum(train.is_converted)/len(train)\n",
    "\n",
    "train, test = train_test_split(train, test_size = 0.4, shuffle = True, random_state = 12345)\n",
    "\n",
    "original_train = pd.read_csv('conversion_train_etc_duplicate.csv')\n",
    "original_sub = pd.read_csv('conversion_sub_etc_duplicate.csv')\n",
    "\n",
    "original_test = original_train.iloc[test.index]\n",
    "original_train = original_train.iloc[train.index]\n",
    "\n",
    "########################### mean_customer_idx 컬럼 생성 ###########################################\n",
    "original_train['customer_idx_cnt'] = original_train.groupby(['customer_idx'])['customer_idx'].transform('count')\n",
    "\n",
    "#빈도 수 (customer_idx_cnt)가 5 이하인 데이터는 customer_idx를 etc로 통일\n",
    "n = 5\n",
    "for i in range(1,n+1):\n",
    "    original_train.loc[original_train['customer_idx_cnt'] == i,'customer_idx'] = 'etc'\n",
    "\n",
    "# customer_idx 별 평균 영업 성공률 계산한 mean_customer_idx 추가\n",
    "original_train['mean_customer_idx'] = original_train.groupby('customer_idx')['is_converted'].transform('mean')\n",
    "\n",
    "original_train.mean_customer_idx = [converted_mean if original_train.customer_idx.iloc[i] == \"etc\" else original_train.mean_customer_idx.iloc[i] for i in range(len(original_train))]\n",
    "mean_customer_idx_unique = original_train.drop_duplicates(subset=['customer_idx'])[['customer_idx', 'mean_customer_idx']]\n",
    "\n",
    "original_test = original_test.merge(mean_customer_idx_unique, on='customer_idx', how='left')\n",
    "original_test['mean_customer_idx'].fillna(original_train[original_train['customer_idx']=='etc']['mean_customer_idx'].iloc[0],inplace= True)\n",
    "\n",
    "original_sub = original_sub.merge(mean_customer_idx_unique, on='customer_idx', how='left')\n",
    "original_sub['mean_customer_idx'].fillna(original_train[original_train['customer_idx']=='etc']['mean_customer_idx'].iloc[0],inplace= True)\n",
    "\n",
    "##################### mean_lead_owner 컬럼 생성 ###################################\n",
    "original_train['lead_owner_cnt'] = original_train.groupby(['lead_owner'])['is_converted'].transform('count')\n",
    "\n",
    "#빈도 수 (lead_owner_cnt)가 5 이하인 데이터는 lead_owner를 etc로 통일\n",
    "n = 5\n",
    "for i in range(1,n+1):\n",
    "    original_train.loc[original_train['lead_owner_cnt'] == i,'lead_owner'] = 'etc'\n",
    "\n",
    "# lead_owner 별 평균 영업 성공률 계산한 mean_lead_owner 추가\n",
    "original_train['mean_lead_owner'] = original_train.groupby('lead_owner')['is_converted'].transform('mean')\n",
    "original_train.mean_lead_owner = [converted_mean if original_train.lead_owner.iloc[i] == \"etc\" else original_train.mean_lead_owner.iloc[i] for i in range(len(original_train))]\n",
    "mean_customer_idx_unique = original_train.drop_duplicates(subset=['lead_owner'])[['lead_owner', 'mean_lead_owner']]\n",
    "\n",
    "original_test = original_test.merge(mean_customer_idx_unique,on='lead_owner',how='left')\n",
    "original_test['mean_lead_owner'].fillna(original_train[original_train['lead_owner']=='etc']['mean_lead_owner'].iloc[0],inplace= True)\n",
    "\n",
    "original_sub = original_sub.merge(mean_customer_idx_unique,on='lead_owner',how='left')\n",
    "original_sub['mean_lead_owner'].fillna(original_train[original_train['lead_owner']=='etc']['mean_lead_owner'].iloc[0],inplace= True)\n",
    "\n",
    "##################### corporate_converted_ratio 컬럼 생성 ###################################\n",
    "table = original_train.value_counts([\"response_corporate\",\"is_converted\"]).reset_index().pivot(index = \"is_converted\", columns = \"response_corporate\", values = 0).T\n",
    "table[\"Total\"] = table.sum(axis = 1)\n",
    "table[\"True_per\"] = table[True]/table[\"Total\"]\n",
    "table = table.fillna(0)\n",
    "table = table.reset_index()[[\"response_corporate\", \"True_per\"]].set_index(\"response_corporate\").T.to_dict()\n",
    "original_train['corporate_converted_ratio'] = [table[i][\"True_per\"] if i in table.keys() else converted_mean for i in original_train.response_corporate]\n",
    "original_test['corporate_converted_ratio'] = [table[i][\"True_per\"] if i in table.keys() else converted_mean for i in original_test.response_corporate]\n",
    "original_sub['corporate_converted_ratio'] = [table[i][\"True_per\"] if i in table.keys() else converted_mean for i in original_sub.response_corporate]\n",
    "\n",
    "original_train = original_train.reset_index(drop = True)\n",
    "original_test = original_test.reset_index(drop = True)\n",
    "train = train.reset_index(drop = True)\n",
    "test = test.reset_index(drop = True)\n",
    "\n",
    "train.mean_customer_idx = original_train.mean_customer_idx\n",
    "test.mean_customer_idx = original_test.mean_customer_idx\n",
    "submission.mean_customer_idx = original_sub.mean_customer_idx\n",
    "\n",
    "train.mean_lead_owner = original_train.mean_lead_owner\n",
    "test.mean_lead_owner = original_test.mean_lead_owner\n",
    "submission.mean_lead_owner = original_sub.mean_lead_owner\n",
    "\n",
    "train.corporate_converted_ratio = original_train.corporate_converted_ratio\n",
    "test.corporate_converted_ratio = original_test.corporate_converted_ratio\n",
    "submission.corporate_converted_ratio = original_sub.corporate_converted_ratio\n",
    "\n",
    "train['inquiry_type'] = original_train.inquiry_type\n",
    "test['inquiry_type'] = original_test.inquiry_type\n",
    "submission['inquiry_type'] = original_sub.inquiry_type\n",
    "\n",
    "\n",
    "### 학습\n",
    "train = train.drop([\"inquiry_type\",'lead_desc_length','expected_timeline','ver_cus',\"bant_submit\",\"historical_existing_cnt\",\"id_strategic_ver\", \"it_strategic_ver\",\"response_corporate\",\"customer_country\", \"customer_job\",\"customer_position\",'ver_win_rate_x','ver_win_ratio_per_bu', \"inquiry_type_isna\"], axis =1)\n",
    "test = test.drop([\"inquiry_type\",'lead_desc_length','expected_timeline','ver_cus',\"bant_submit\",\"historical_existing_cnt\",\"id_strategic_ver\", \"it_strategic_ver\",\"response_corporate\",\"customer_country\", \"customer_job\",\"customer_position\",'ver_win_rate_x','ver_win_ratio_per_bu', \"inquiry_type_isna\"], axis =1)\n",
    "submission = submission.drop([\"inquiry_type\",'lead_desc_length','expected_timeline','ver_cus',\"bant_submit\",\"historical_existing_cnt\",\"id_strategic_ver\", \"it_strategic_ver\",\"response_corporate\",\"customer_country\", \"customer_job\",\"customer_position\",'ver_win_rate_x','ver_win_ratio_per_bu', \"inquiry_type_isna\"], axis =1)\n",
    "\n",
    "target = [\"is_converted\"]\n",
    "categorical = [i for i in train.columns if train[i].dtype == \"O\"]\n",
    "numeric = train.columns.drop(categorical + target).tolist()\n",
    "\n",
    "x_train = train[train.columns.drop(target)]\n",
    "y_train = train[target]\n",
    "\n",
    "x_train[\"구분\"] = \"train\"\n",
    "test[\"구분\"] = \"test\"\n",
    "submission[\"구분\"] = \"sub\"\n",
    "new = pd.concat([x_train, test, submission]).reset_index(drop = True)\n",
    "\n",
    "# 라벨 인코딩\n",
    "c = []\n",
    "cat_idxs = []\n",
    "cat_dims = []\n",
    "for i in range(x_train.shape[1]):\n",
    "    if x_train.columns[i] in categorical and x_train.columns[i] != \"is_converted\" and x_train.columns[i] != \"구분\":\n",
    "        encoder = LabelEncoder()\n",
    "        new[x_train.columns[i]] = encoder.fit_transform(new[x_train.columns[i]])\n",
    "        cat_idxs.append(i)\n",
    "        cat_dims.append(len(encoder.classes_))\n",
    "    \n",
    "    \n",
    "x_train = new[new.구분 == \"train\"].drop([\"구분\"], axis = 1).reset_index(drop = True)\n",
    "test = new[new.구분 == \"test\"].drop([\"구분\"], axis = 1).reset_index(drop = True)\n",
    "submission = new[new.구분 == \"sub\"].drop([\"구분\"], axis = 1).reset_index(drop = True)\n",
    "\n",
    "x_train = x_train.drop([\"id\", \"is_converted\"], axis = 1)\n",
    "test = test.drop([\"id\"], axis = 1)\n",
    "submission = submission.drop(target, axis = 1)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "t = scaler.fit_transform(x_train[categorical+numeric])\n",
    "x_train[categorical+numeric] = pd.DataFrame(t, columns = categorical+numeric)\n",
    "\n",
    "t = scaler.transform(test[categorical+numeric])\n",
    "test[categorical+numeric] = pd.DataFrame(t, columns = categorical+numeric)\n",
    "\n",
    "\n",
    "t = scaler.transform(submission[categorical+numeric])\n",
    "submission[categorical+numeric] = pd.DataFrame(t, columns = categorical+numeric)\n",
    "\n",
    "x_test = test.drop(target, axis = 1)\n",
    "y_test = test[target]\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size = 0.5, random_state = 42, shuffle = True)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "def objective(trial):\n",
    "    cat_emb_dim = trial.suggest_int('cat_emb_dim', 3, 10)\n",
    "    lr = trial.suggest_loguniform('lr', 0.001, 0.0017)\n",
    "    step_size = trial.suggest_int('step_size', 30, 40)\n",
    "    gamma = trial.suggest_uniform('gamma', 0.6, 0.8)\n",
    "    patience = trial.suggest_int('patience', 2, 5)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    virtual_batch_size = trial.suggest_categorical('virtual_batch_size', [32, 64, 128])\n",
    "    weights = trial.suggest_categorical('weights', [0,1])\n",
    "    \n",
    "    clf = TabNetClassifier(cat_idxs=cat_idxs,\n",
    "                       cat_dims=cat_dims,\n",
    "                       cat_emb_dim=cat_emb_dim,\n",
    "                       optimizer_fn=torch.optim.Adam,\n",
    "                       optimizer_params=dict(lr=lr),\n",
    "                       scheduler_params={\"step_size\":step_size,\n",
    "                                         \"gamma\":gamma},\n",
    "                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                       mask_type='entmax' # \"sparsemax\", entmax\n",
    "                      )\n",
    "    \n",
    "    clf.fit(\n",
    "        X_train=x_train.values, y_train=y_train.values.reshape(-1)*1,\n",
    "        eval_set=[(x_train.values, y_train.values.reshape(-1)*1),(x_val.values, np.asarray(y_val.values.reshape(-1)*1, dtype = int))],\n",
    "        eval_name=[\"train\", 'valid'],\n",
    "        eval_metric=[\"accuracy\"],\n",
    "        max_epochs=30 , patience=patience,\n",
    "        batch_size=batch_size, virtual_batch_size=virtual_batch_size,\n",
    "        num_workers=1,\n",
    "        weights=weights,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    \n",
    "    preds = clf.predict(x_test.values)\n",
    "    \n",
    "    f1 = f1_score(preds, np.asarray(y_test.values.reshape(-1)*1, dtype = int))\n",
    "    return f1\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "start = time.time()\n",
    "study.optimize(objective, n_trials=50)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"complete_train_duplication.csv\")\n",
    "submission = pd.read_csv(\"complete_submission_duplication.csv\")\n",
    "\n",
    "converted_mean = sum(train.is_converted)/len(train)\n",
    "\n",
    "train, test = train_test_split(train, test_size = 0.4, shuffle = True, random_state = 12345)\n",
    "\n",
    "original_train = pd.read_csv('conversion_train_etc_duplicate.csv')\n",
    "original_sub = pd.read_csv('conversion_sub_etc_duplicate.csv')\n",
    "\n",
    "original_test = original_train.iloc[test.index]\n",
    "original_train = original_train.iloc[train.index]\n",
    "\n",
    "########################### mean_customer_idx 컬럼 생성 ###########################################\n",
    "original_train['customer_idx_cnt'] = original_train.groupby(['customer_idx'])['customer_idx'].transform('count')\n",
    "\n",
    "#빈도 수 (customer_idx_cnt)가 5 이하인 데이터는 customer_idx를 etc로 통일\n",
    "n = 5\n",
    "for i in range(1,n+1):\n",
    "    original_train.loc[original_train['customer_idx_cnt'] == i,'customer_idx'] = 'etc'\n",
    "\n",
    "# customer_idx 별 평균 영업 성공률 계산한 mean_customer_idx 추가\n",
    "original_train['mean_customer_idx'] = original_train.groupby('customer_idx')['is_converted'].transform('mean')\n",
    "\n",
    "original_train.mean_customer_idx = [converted_mean if original_train.customer_idx.iloc[i] == \"etc\" else original_train.mean_customer_idx.iloc[i] for i in range(len(original_train))]\n",
    "mean_customer_idx_unique = original_train.drop_duplicates(subset=['customer_idx'])[['customer_idx', 'mean_customer_idx']]\n",
    "\n",
    "original_test = original_test.merge(mean_customer_idx_unique, on='customer_idx', how='left')\n",
    "original_test['mean_customer_idx'].fillna(original_train[original_train['customer_idx']=='etc']['mean_customer_idx'].iloc[0],inplace= True)\n",
    "\n",
    "original_sub = original_sub.merge(mean_customer_idx_unique, on='customer_idx', how='left')\n",
    "original_sub['mean_customer_idx'].fillna(original_train[original_train['customer_idx']=='etc']['mean_customer_idx'].iloc[0],inplace= True)\n",
    "\n",
    "##################### mean_lead_owner 컬럼 생성 ###################################\n",
    "original_train['lead_owner_cnt'] = original_train.groupby(['lead_owner'])['is_converted'].transform('count')\n",
    "\n",
    "#빈도 수 (lead_owner_cnt)가 5 이하인 데이터는 lead_owner를 etc로 통일\n",
    "n = 5\n",
    "for i in range(1,n+1):\n",
    "    original_train.loc[original_train['lead_owner_cnt'] == i,'lead_owner'] = 'etc'\n",
    "\n",
    "# lead_owner 별 평균 영업 성공률 계산한 mean_lead_owner 추가\n",
    "original_train['mean_lead_owner'] = original_train.groupby('lead_owner')['is_converted'].transform('mean')\n",
    "original_train.mean_lead_owner = [converted_mean if original_train.lead_owner.iloc[i] == \"etc\" else original_train.mean_lead_owner.iloc[i] for i in range(len(original_train))]\n",
    "mean_customer_idx_unique = original_train.drop_duplicates(subset=['lead_owner'])[['lead_owner', 'mean_lead_owner']]\n",
    "\n",
    "original_test = original_test.merge(mean_customer_idx_unique,on='lead_owner',how='left')\n",
    "original_test['mean_lead_owner'].fillna(original_train[original_train['lead_owner']=='etc']['mean_lead_owner'].iloc[0],inplace= True)\n",
    "\n",
    "original_sub = original_sub.merge(mean_customer_idx_unique,on='lead_owner',how='left')\n",
    "original_sub['mean_lead_owner'].fillna(original_train[original_train['lead_owner']=='etc']['mean_lead_owner'].iloc[0],inplace= True)\n",
    "\n",
    "##################### corporate_converted_ratio 컬럼 생성 ###################################\n",
    "table = original_train.value_counts([\"response_corporate\",\"is_converted\"]).reset_index().pivot(index = \"is_converted\", columns = \"response_corporate\", values = 0).T\n",
    "table[\"Total\"] = table.sum(axis = 1)\n",
    "table[\"True_per\"] = table[True]/table[\"Total\"]\n",
    "table = table.fillna(0)\n",
    "table = table.reset_index()[[\"response_corporate\", \"True_per\"]].set_index(\"response_corporate\").T.to_dict()\n",
    "original_train['corporate_converted_ratio'] = [table[i][\"True_per\"] if i in table.keys() else converted_mean for i in original_train.response_corporate]\n",
    "original_test['corporate_converted_ratio'] = [table[i][\"True_per\"] if i in table.keys() else converted_mean for i in original_test.response_corporate]\n",
    "original_sub['corporate_converted_ratio'] = [table[i][\"True_per\"] if i in table.keys() else converted_mean for i in original_sub.response_corporate]\n",
    "\n",
    "original_train = original_train.reset_index(drop = True)\n",
    "original_test = original_test.reset_index(drop = True)\n",
    "train = train.reset_index(drop = True)\n",
    "test = test.reset_index(drop = True)\n",
    "\n",
    "train.mean_customer_idx = original_train.mean_customer_idx\n",
    "test.mean_customer_idx = original_test.mean_customer_idx\n",
    "submission.mean_customer_idx = original_sub.mean_customer_idx\n",
    "\n",
    "train.mean_lead_owner = original_train.mean_lead_owner\n",
    "test.mean_lead_owner = original_test.mean_lead_owner\n",
    "submission.mean_lead_owner = original_sub.mean_lead_owner\n",
    "\n",
    "train.corporate_converted_ratio = original_train.corporate_converted_ratio\n",
    "test.corporate_converted_ratio = original_test.corporate_converted_ratio\n",
    "submission.corporate_converted_ratio = original_sub.corporate_converted_ratio\n",
    "\n",
    "\n",
    "### 학습\n",
    "train = train.drop(['lead_desc_length','expected_timeline','ver_cus',\"bant_submit\",\"historical_existing_cnt\",\"id_strategic_ver\", \"it_strategic_ver\",\"response_corporate\",\"customer_country\", \"customer_job\",\"customer_position\",'ver_win_rate_x','ver_win_ratio_per_bu', \"inquiry_type_isna\"], axis =1)\n",
    "test = test.drop(['lead_desc_length','expected_timeline','ver_cus',\"bant_submit\",\"historical_existing_cnt\",\"id_strategic_ver\", \"it_strategic_ver\",\"response_corporate\",\"customer_country\", \"customer_job\",\"customer_position\",'ver_win_rate_x','ver_win_ratio_per_bu', \"inquiry_type_isna\"], axis =1)\n",
    "submission = submission.drop(['lead_desc_length','expected_timeline','ver_cus',\"bant_submit\",\"historical_existing_cnt\",\"id_strategic_ver\", \"it_strategic_ver\",\"response_corporate\",\"customer_country\", \"customer_job\",\"customer_position\",'ver_win_rate_x','ver_win_ratio_per_bu', \"inquiry_type_isna\"], axis =1)\n",
    "\n",
    "target = [\"is_converted\"]\n",
    "categorical = [i for i in train.columns if train[i].dtype == \"O\"]\n",
    "numeric = train.columns.drop(categorical + target).tolist()\n",
    "\n",
    "x_train = train[train.columns.drop(target)]\n",
    "y_train = train[target]\n",
    "\n",
    "x_train[\"구분\"] = \"train\"\n",
    "test[\"구분\"] = \"test\"\n",
    "submission[\"구분\"] = \"sub\"\n",
    "new = pd.concat([x_train, test, submission]).reset_index(drop = True)\n",
    "\n",
    "# 라벨 인코딩\n",
    "c = []\n",
    "cat_idxs = []\n",
    "cat_dims = []\n",
    "for i in range(x_train.shape[1]):\n",
    "    if x_train.columns[i] in categorical and x_train.columns[i] != \"is_converted\" and x_train.columns[i] != \"구분\":\n",
    "        encoder = LabelEncoder()\n",
    "        new[x_train.columns[i]] = encoder.fit_transform(new[x_train.columns[i]])\n",
    "        cat_idxs.append(i)\n",
    "        cat_dims.append(len(encoder.classes_))\n",
    "    \n",
    "    \n",
    "x_train = new[new.구분 == \"train\"].drop([\"구분\"], axis = 1).reset_index(drop = True)\n",
    "test = new[new.구분 == \"test\"].drop([\"구분\"], axis = 1).reset_index(drop = True)\n",
    "submission = new[new.구분 == \"sub\"].drop([\"구분\"], axis = 1).reset_index(drop = True)\n",
    "\n",
    "x_train = x_train.drop([\"id\", \"is_converted\"], axis = 1)\n",
    "test = test.drop([\"id\"], axis = 1)\n",
    "submission = submission.drop(target, axis = 1)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "t = scaler.fit_transform(x_train[categorical+numeric])\n",
    "x_train[categorical+numeric] = pd.DataFrame(t, columns = categorical+numeric)\n",
    "\n",
    "t = scaler.transform(test[categorical+numeric])\n",
    "test[categorical+numeric] = pd.DataFrame(t, columns = categorical+numeric)\n",
    "\n",
    "\n",
    "t = scaler.transform(submission[categorical+numeric])\n",
    "submission[categorical+numeric] = pd.DataFrame(t, columns = categorical+numeric)\n",
    "\n",
    "x_test = test.drop(target, axis = 1)\n",
    "y_test = test[target]\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size = 0.5, random_state = 42, shuffle = True)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "best = study.best_trial.params\n",
    "# 베스트 스코어로 적합 시켜보기\n",
    "clf = TabNetClassifier(cat_idxs=cat_idxs,\n",
    "                       cat_dims=cat_dims,\n",
    "                       cat_emb_dim=best[\"cat_emb_dim\"],\n",
    "                       optimizer_fn=torch.optim.Adam,\n",
    "                       optimizer_params=dict(lr=best[\"lr\"]),\n",
    "                       scheduler_params={\"step_size\":best[\"step_size\"],\n",
    "                                         \"gamma\":best[\"gamma\"]},\n",
    "                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                       mask_type='entmax' # \"sparsemax\", entmax\n",
    "                      )\n",
    "\n",
    "clf.fit(\n",
    "    X_train=x_train.values, y_train=y_train.values.reshape(-1)*1,\n",
    "    eval_set=[(x_train.values, y_train.values.reshape(-1)*1), (x_val.values, np.asarray(y_val.values.reshape(-1)*1, dtype = int))],\n",
    "    eval_name=['train','valid'],\n",
    "    eval_metric=[\"accuracy\"],\n",
    "    max_epochs=20 , patience=best[\"patience\"],\n",
    "    batch_size=best[\"batch_size\"], virtual_batch_size=best[\"virtual_batch_size\"],\n",
    "    num_workers=0,\n",
    "    weights=1,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "preds = clf.predict(x_test.values)\n",
    "f1_score(preds, np.asarray(y_test.values.reshape(-1)*1, dtype = int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"complete_train_duplication.csv\")\n",
    "submission = pd.read_csv(\"complete_submission_duplication.csv\")\n",
    "\n",
    "converted_mean = sum(train.is_converted)/len(train)\n",
    "\n",
    "original_train = pd.read_csv('conversion_train_etc_duplicate.csv')\n",
    "original_sub = pd.read_csv('conversion_sub_etc_duplicate.csv')\n",
    "\n",
    "########################### mean_customer_idx 컬럼 생성 ###########################################\n",
    "original_train['customer_idx_cnt'] = original_train.groupby(['customer_idx'])['customer_idx'].transform('count')\n",
    "\n",
    "#빈도 수 (customer_idx_cnt)가 5 이하인 데이터는 customer_idx를 etc로 통일\n",
    "n = 5\n",
    "for i in range(1,n+1):\n",
    "    original_train.loc[original_train['customer_idx_cnt'] == i,'customer_idx'] = 'etc'\n",
    "\n",
    "# customer_idx 별 평균 영업 성공률 계산한 mean_customer_idx 추가\n",
    "original_train['mean_customer_idx'] = original_train.groupby('customer_idx')['is_converted'].transform('mean')\n",
    "\n",
    "original_train.mean_customer_idx = [converted_mean if original_train.customer_idx.iloc[i] == \"etc\" else original_train.mean_customer_idx.iloc[i] for i in range(len(original_train))]\n",
    "mean_customer_idx_unique = original_train.drop_duplicates(subset=['customer_idx'])[['customer_idx', 'mean_customer_idx']]\n",
    "\n",
    "original_sub = original_sub.merge(mean_customer_idx_unique, on='customer_idx', how='left')\n",
    "original_sub['mean_customer_idx'].fillna(original_train[original_train['customer_idx']=='etc']['mean_customer_idx'].iloc[0],inplace= True)\n",
    "\n",
    "##################### mean_lead_owner 컬럼 생성 ###################################\n",
    "original_train['lead_owner_cnt'] = original_train.groupby(['lead_owner'])['is_converted'].transform('count')\n",
    "\n",
    "#빈도 수 (lead_owner_cnt)가 5 이하인 데이터는 lead_owner를 etc로 통일\n",
    "n = 5\n",
    "for i in range(1,n+1):\n",
    "    original_train.loc[original_train['lead_owner_cnt'] == i,'lead_owner'] = 'etc'\n",
    "\n",
    "# lead_owner 별 평균 영업 성공률 계산한 mean_lead_owner 추가\n",
    "original_train['mean_lead_owner'] = original_train.groupby('lead_owner')['is_converted'].transform('mean')\n",
    "original_train.mean_lead_owner = [converted_mean if original_train.lead_owner.iloc[i] == \"etc\" else original_train.mean_lead_owner.iloc[i] for i in range(len(original_train))]\n",
    "mean_customer_idx_unique = original_train.drop_duplicates(subset=['lead_owner'])[['lead_owner', 'mean_lead_owner']]\n",
    "\n",
    "original_sub = original_sub.merge(mean_customer_idx_unique,on='lead_owner',how='left')\n",
    "original_sub['mean_lead_owner'].fillna(original_train[original_train['lead_owner']=='etc']['mean_lead_owner'].iloc[0],inplace= True)\n",
    "\n",
    "##################### corporate_converted_ratio 컬럼 생성 ###################################\n",
    "table = original_train.value_counts([\"response_corporate\",\"is_converted\"]).reset_index().pivot(index = \"is_converted\", columns = \"response_corporate\", values = 0).T\n",
    "table[\"Total\"] = table.sum(axis = 1)\n",
    "table[\"True_per\"] = table[True]/table[\"Total\"]\n",
    "table = table.fillna(0)\n",
    "table = table.reset_index()[[\"response_corporate\", \"True_per\"]].set_index(\"response_corporate\").T.to_dict()\n",
    "original_train['corporate_converted_ratio'] = [table[i][\"True_per\"] if i in table.keys() else converted_mean for i in original_train.response_corporate]\n",
    "original_sub['corporate_converted_ratio'] = [table[i][\"True_per\"] if i in table.keys() else converted_mean for i in original_sub.response_corporate]\n",
    "\n",
    "train.mean_customer_idx = original_train.mean_customer_idx\n",
    "submission.mean_customer_idx = original_sub.mean_customer_idx\n",
    "\n",
    "train.mean_lead_owner = original_train.mean_lead_owner\n",
    "submission.mean_lead_owner = original_sub.mean_lead_owner\n",
    "\n",
    "train.corporate_converted_ratio = original_train.corporate_converted_ratio\n",
    "submission.corporate_converted_ratio = original_sub.corporate_converted_ratio\n",
    "\n",
    "### 학습\n",
    "train = train.drop(['lead_desc_length','expected_timeline','ver_cus',\"bant_submit\",\"historical_existing_cnt\",\"id_strategic_ver\", \"it_strategic_ver\",\"response_corporate\",\"customer_country\", \"customer_job\",\"customer_position\",'ver_win_rate_x','ver_win_ratio_per_bu', \"inquiry_type_isna\"], axis =1)\n",
    "submission = submission.drop(['lead_desc_length','expected_timeline','ver_cus',\"bant_submit\",\"historical_existing_cnt\",\"id_strategic_ver\", \"it_strategic_ver\",\"response_corporate\",\"customer_country\", \"customer_job\",\"customer_position\",'ver_win_rate_x','ver_win_ratio_per_bu', \"inquiry_type_isna\"], axis =1)\n",
    "\n",
    "target = [\"is_converted\"]\n",
    "categorical = [i for i in train.columns if train[i].dtype == \"O\"]\n",
    "numeric = train.columns.drop(categorical + target).tolist()\n",
    "\n",
    "train[\"구분\"] = \"train\"\n",
    "submission[\"구분\"] = \"sub\"\n",
    "new = pd.concat([train, submission]).reset_index(drop = True)\n",
    "\n",
    "# 라벨 인코딩\n",
    "c = []\n",
    "cat_idxs = []\n",
    "cat_dims = []\n",
    "for i in range(train.shape[1]):\n",
    "    if train.columns[i] in categorical and train.columns[i] != \"is_converted\" and train.columns[i] != \"구분\":\n",
    "        encoder = LabelEncoder()\n",
    "        new[train.columns[i]] = encoder.fit_transform(new[train.columns[i]])\n",
    "        cat_idxs.append(i)\n",
    "        cat_dims.append(len(encoder.classes_))\n",
    "    \n",
    "    \n",
    "train = new[new.구분 == \"train\"].drop([\"구분\"], axis = 1).reset_index(drop = True)\n",
    "submission = new[new.구분 == \"sub\"].drop([\"구분\"], axis = 1).reset_index(drop = True)\n",
    "\n",
    "train = train.drop([\"id\"], axis = 1)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "t = scaler.fit_transform(train[categorical+numeric])\n",
    "train[categorical+numeric] = pd.DataFrame(t, columns = categorical+numeric)\n",
    "\n",
    "t = scaler.transform(submission[categorical+numeric])\n",
    "submission[categorical+numeric] = pd.DataFrame(t, columns = categorical+numeric)\n",
    "\n",
    "X = train[train.columns.drop(target)]\n",
    "y = train[target]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, shuffle = True)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "best = {'cat_emb_dim': 6,\n",
    " 'lr': 0.0010841620912501022,\n",
    " 'step_size': 36,\n",
    " 'gamma': 0.6202850279587523,\n",
    " 'patience': 21,\n",
    " 'batch_size': 32,\n",
    " 'virtual_batch_size': 64,\n",
    " 'weights': 0}\n",
    "\n",
    "clf = TabNetClassifier(cat_idxs=cat_idxs,\n",
    "                       cat_dims=cat_dims,\n",
    "                       cat_emb_dim=best[\"cat_emb_dim\"],\n",
    "                       optimizer_fn=torch.optim.Adam,\n",
    "                       optimizer_params=dict(lr=best[\"lr\"]),\n",
    "                       scheduler_params={\"step_size\":best[\"step_size\"],\n",
    "                                         \"gamma\":best[\"gamma\"]},\n",
    "                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                       mask_type='entmax' # \"sparsemax\", entmax\n",
    "                      )\n",
    "\n",
    "clf.fit(\n",
    "    X_train=x_train.values, y_train=y_train.values.reshape(-1)*1,\n",
    "    eval_set=[(x_train.values, y_train.values.reshape(-1)*1), (x_test.values, y_test.values.reshape(-1)*1)],\n",
    "    eval_name=['train','test'],\n",
    "    eval_metric=[\"accuracy\"],\n",
    "    max_epochs=5 , patience=best[\"patience\"],\n",
    "    batch_size=best[\"batch_size\"], virtual_batch_size=best[\"virtual_batch_size\"],\n",
    "    num_workers=0,\n",
    "    weights=1,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "preds = clf.predict(submission.drop([\"id\", \"is_converted\"], axis = 1).values)\n",
    "\n",
    "submission = pd.read_csv(\"submission.csv\")\n",
    "submission['is_converted'] = preds\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 전처리가 완료된 데이터 (모델 돌릴 때마다 전처리하고 싶지 않아서 csv파일로 저장해둔 것)\n",
    "train = pd.read_csv(\"complete_train_duplication.csv\")\n",
    "submission = pd.read_csv(\"complete_submission_duplication.csv\")\n",
    "\n",
    "# 전체 평균\n",
    "converted_mean = sum(train.is_converted)/len(train)\n",
    "\n",
    "# train, test로 미리 나눔\n",
    "# -> 영업 성공률이 쓰이는 변수는 분리를 해서 train으로 변수를 만들고 그에 따라 test에 집어넣어야 실제 제출하는 submission이랑 비슷하다고 판단\n",
    "train, test = train_test_split(train, test_size = 0.4, shuffle = True, random_state = 12345)\n",
    "\n",
    "# 영업 성공률이 쓰이는 세 변수를 재생성\n",
    "# -> 빈도수가 5 이하거나 결측치인 경우 기존에는 etc로 묶어 etc의 영업성공률 평균으로 계산\n",
    "# => 수정 버전은 etc의 영업성공률이 아닌 전체 train의 영업 성공률로 변환\n",
    "\n",
    "# 변수 재생성을 위해 전처리가 되지 않은 파일 read (중복행 제거 및 customer_country만 변경된 데이터)\n",
    "original_train = pd.read_csv('conversion_train_etc_duplicate.csv')\n",
    "original_sub = pd.read_csv('conversion_sub_etc_duplicate.csv')\n",
    "\n",
    "# 위 train, test로 나눈데로 index를 이용해 똑같이 나눔\n",
    "original_test = original_train.iloc[test.index]\n",
    "original_train = original_train.iloc[train.index]\n",
    "\n",
    "########################### mean_customer_idx 컬럼 생성 ###########################################\n",
    "original_train['customer_idx_cnt'] = original_train.groupby(['customer_idx'])['customer_idx'].transform('count')\n",
    "\n",
    "#빈도 수 (customer_idx_cnt)가 5 이하인 데이터는 customer_idx를 etc로 통일\n",
    "n = 5\n",
    "for i in range(1,n+1):\n",
    "    original_train.loc[original_train['customer_idx_cnt'] == i,'customer_idx'] = 'etc'\n",
    "\n",
    "# customer_idx 별 평균 영업 성공률 계산한 mean_customer_idx 추가\n",
    "original_train['mean_customer_idx'] = original_train.groupby('customer_idx')['is_converted'].transform('mean')\n",
    "\n",
    "original_train.mean_customer_idx = [converted_mean if original_train.customer_idx.iloc[i] == \"etc\" else original_train.mean_customer_idx.iloc[i] for i in range(len(original_train))]\n",
    "mean_customer_idx_unique = original_train.drop_duplicates(subset=['customer_idx'])[['customer_idx', 'mean_customer_idx']]\n",
    "\n",
    "# train을 기준으로 test와 submission에 적용\n",
    "original_test = original_test.merge(mean_customer_idx_unique, on='customer_idx', how='left')\n",
    "original_test['mean_customer_idx'].fillna(original_train[original_train['customer_idx']=='etc']['mean_customer_idx'].iloc[0],inplace= True)\n",
    "\n",
    "original_sub = original_sub.merge(mean_customer_idx_unique, on='customer_idx', how='left')\n",
    "original_sub['mean_customer_idx'].fillna(original_train[original_train['customer_idx']=='etc']['mean_customer_idx'].iloc[0],inplace= True)\n",
    "\n",
    "##################### mean_lead_owner 컬럼 생성 ###################################\n",
    "original_train['lead_owner_cnt'] = original_train.groupby(['lead_owner'])['is_converted'].transform('count')\n",
    "\n",
    "#빈도 수 (lead_owner_cnt)가 5 이하인 데이터는 lead_owner를 etc로 통일\n",
    "n = 5\n",
    "for i in range(1,n+1):\n",
    "    original_train.loc[original_train['lead_owner_cnt'] == i,'lead_owner'] = 'etc'\n",
    "\n",
    "# lead_owner 별 평균 영업 성공률 계산한 mean_lead_owner 추가\n",
    "original_train['mean_lead_owner'] = original_train.groupby('lead_owner')['is_converted'].transform('mean')\n",
    "original_train.mean_lead_owner = [converted_mean if original_train.lead_owner.iloc[i] == \"etc\" else original_train.mean_lead_owner.iloc[i] for i in range(len(original_train))]\n",
    "mean_customer_idx_unique = original_train.drop_duplicates(subset=['lead_owner'])[['lead_owner', 'mean_lead_owner']]\n",
    "\n",
    "# train을 기준으로 test와 submission에 적용\n",
    "original_test = original_test.merge(mean_customer_idx_unique,on='lead_owner',how='left')\n",
    "original_test['mean_lead_owner'].fillna(original_train[original_train['lead_owner']=='etc']['mean_lead_owner'].iloc[0],inplace= True)\n",
    "\n",
    "original_sub = original_sub.merge(mean_customer_idx_unique,on='lead_owner',how='left')\n",
    "original_sub['mean_lead_owner'].fillna(original_train[original_train['lead_owner']=='etc']['mean_lead_owner'].iloc[0],inplace= True)\n",
    "\n",
    "##################### corporate_converted_ratio 컬럼 생성 ###################################\n",
    "table = original_train.value_counts([\"response_corporate\",\"is_converted\"]).reset_index().pivot(index = \"is_converted\", columns = \"response_corporate\", values = 0).T\n",
    "table[\"Total\"] = table.sum(axis = 1)\n",
    "table[\"True_per\"] = table[True]/table[\"Total\"]\n",
    "table = table.fillna(0)\n",
    "table = table.reset_index()[[\"response_corporate\", \"True_per\"]].set_index(\"response_corporate\").T.to_dict()\n",
    "original_train['corporate_converted_ratio'] = [table[i][\"True_per\"] if i in table.keys() else converted_mean for i in original_train.response_corporate]\n",
    "\n",
    "# train을 기준으로 test와 submission에 적용\n",
    "original_test['corporate_converted_ratio'] = [table[i][\"True_per\"] if i in table.keys() else converted_mean for i in original_test.response_corporate]\n",
    "original_sub['corporate_converted_ratio'] = [table[i][\"True_per\"] if i in table.keys() else converted_mean for i in original_sub.response_corporate]\n",
    "\n",
    "# index 재정렬\n",
    "original_train = original_train.reset_index(drop = True)\n",
    "original_test = original_test.reset_index(drop = True)\n",
    "train = train.reset_index(drop = True)\n",
    "test = test.reset_index(drop = True)\n",
    "\n",
    "# 바꾼 세 변수를 train, test, submission에 각각 적용\n",
    "train.mean_customer_idx = original_train.mean_customer_idx\n",
    "test.mean_customer_idx = original_test.mean_customer_idx\n",
    "submission.mean_customer_idx = original_sub.mean_customer_idx\n",
    "\n",
    "train.mean_lead_owner = original_train.mean_lead_owner\n",
    "test.mean_lead_owner = original_test.mean_lead_owner\n",
    "submission.mean_lead_owner = original_sub.mean_lead_owner\n",
    "\n",
    "train.corporate_converted_ratio = original_train.corporate_converted_ratio\n",
    "test.corporate_converted_ratio = original_test.corporate_converted_ratio\n",
    "submission.corporate_converted_ratio = original_sub.corporate_converted_ratio\n",
    "\n",
    "######## 학습\n",
    "# 불필요한 변수 drop\n",
    "train = train.drop(['lead_desc_length','expected_timeline','ver_cus',\"bant_submit\",\"historical_existing_cnt\",\"id_strategic_ver\", \"it_strategic_ver\",\"response_corporate\",\"customer_country\", \"customer_job\",\"customer_position\",'ver_win_rate_x','ver_win_ratio_per_bu', \"inquiry_type_isna\"], axis =1)\n",
    "test = test.drop(['lead_desc_length','expected_timeline','ver_cus',\"bant_submit\",\"historical_existing_cnt\",\"id_strategic_ver\", \"it_strategic_ver\",\"response_corporate\",\"customer_country\", \"customer_job\",\"customer_position\",'ver_win_rate_x','ver_win_ratio_per_bu', \"inquiry_type_isna\"], axis =1)\n",
    "submission = submission.drop(['lead_desc_length','expected_timeline','ver_cus',\"bant_submit\",\"historical_existing_cnt\",\"id_strategic_ver\", \"it_strategic_ver\",\"response_corporate\",\"customer_country\", \"customer_job\",\"customer_position\",'ver_win_rate_x','ver_win_ratio_per_bu', \"inquiry_type_isna\"], axis =1)\n",
    "\n",
    "# 범주형, 수치형 변수로 나눔\n",
    "target = [\"is_converted\"]\n",
    "categorical = [i for i in train.columns if train[i].dtype == \"O\"]\n",
    "numeric = train.columns.drop(categorical + target).tolist()\n",
    "\n",
    "train[\"구분\"] = \"train\"\n",
    "test[\"구분\"] = \"test\"\n",
    "submission[\"구분\"] = \"sub\"\n",
    "\n",
    "new = pd.concat([train, test, submission]).reset_index(drop = True)\n",
    "\n",
    "# 인코딩\n",
    "c = []\n",
    "for i in categorical:\n",
    "    encoder = OneHotEncoder(drop='first')\n",
    "    encoder.fit(new[[i]])\n",
    "    new_df = encoder.transform(new[[i]])\n",
    "    \n",
    "    new_df = pd.DataFrame(new_df.toarray(), columns = encoder.get_feature_names_out([i]))\n",
    "    new = new.drop([i], axis = 1)\n",
    "    new = pd.concat([new, new_df], axis = 1)\n",
    "    c += encoder.get_feature_names_out([i]).tolist()\n",
    "\n",
    "categorical = c\n",
    "train = new[new.구분 == \"train\"].drop([\"구분\"], axis = 1).reset_index(drop = True)\n",
    "test = new[new.구분 == \"test\"].drop([\"구분\"], axis = 1).reset_index(drop = True)\n",
    "submission = new[new.구분 == \"sub\"].drop([\"구분\"], axis = 1).reset_index(drop = True)\n",
    "\n",
    "train = train.drop([\"id\"], axis = 1)\n",
    "test = test.drop([\"id\"], axis = 1)\n",
    "    \n",
    "\n",
    "# 결측치 채우기\n",
    "#imputer = KNNImputer(n_neighbors = 3)\n",
    "#t = imputer.fit_transform(new_train[categorical + numeric])\n",
    "#new_train[categorical + numeric] = pd.DataFrame(t, columns = categorical + numeric)\n",
    "\n",
    "# MinMax scaling\n",
    "scaler = MinMaxScaler()\n",
    "t = scaler.fit_transform(train[categorical+numeric])\n",
    "train[categorical+numeric] = pd.DataFrame(t, columns = categorical+numeric)\n",
    "\n",
    "t = scaler.transform(test[categorical+numeric])\n",
    "test[categorical+numeric] = pd.DataFrame(t, columns = categorical+numeric)\n",
    "\n",
    "t = scaler.transform(submission[categorical+numeric])\n",
    "submission[categorical+numeric] = pd.DataFrame(t, columns = categorical+numeric)\n",
    "\n",
    "# train데이터를 x, y로 나눔\n",
    "x_train = train[train.columns.drop(target)]\n",
    "y_train = train[target]\n",
    "\n",
    "# test 데이터를 x, y로 나눔\n",
    "x_test = test[test.columns.drop(target)]\n",
    "y_test = test[target]\n",
    "\n",
    "# test 데이터에서 validation 데이터를 생성 -> 영업 성공률 변수의 영향을 받지 않기 위해서\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size = 0.2, random_state = 42, shuffle = True)\n",
    "\n",
    "# train만 SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parameter_loss = pd.DataFrame([], columns = [\"Parameters\", \"Loss\"])\n",
    "def objective(trial):\n",
    "    # 최적화 시킬 파라미터 후보들\n",
    "    params = []\n",
    "    patience = trial.suggest_int('patience', 1, 3)\n",
    "    min_delta = trial.suggest_categorical('min_delta', [0.004, 0.005, 0.006, 0.007, 0.008])\n",
    "\n",
    "    node1 = trial.suggest_categorical('node1', [16, 32, 64, 128])\n",
    "    dropout1 = trial.suggest_uniform('dropout1', 0, 1)\n",
    "    node2 = trial.suggest_categorical('node2', [16, 32, 64, 128])\n",
    "    dropout2 = trial.suggest_uniform('dropout2', 0, 1)\n",
    "    num_layers = trial.suggest_categorical('num_layers', [\"two\", \"three\"])\n",
    "    if num_layers != \"two\":\n",
    "            node3 = trial.suggest_categorical('node3', [16, 32, 64, 128])\n",
    "            dropout3 = trial.suggest_uniform('dropout3', 0, 1)\n",
    "    lr = trial.suggest_uniform('lr', 0.0001, 0.025)\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_f1_score',\n",
    "                                                      mode = 'min',\n",
    "                                                      patience = patience,\n",
    "                                                      min_delta = min_delta)\n",
    "\n",
    "    # MLP 코드\n",
    "    tf.random.set_seed(89)\n",
    "    initializer = tf.keras.initializers.HeNormal(seed = 89)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(node1, \n",
    "                                    activation = 'relu', kernel_initializer=initializer))\n",
    "    model.add(tf.keras.layers.Dropout(dropout1))\n",
    "    if num_layers == \"two\":\n",
    "        model.add(tf.keras.layers.Dense(node2,  \n",
    "                                        activation = 'relu', kernel_initializer=initializer))\n",
    "        model.add(tf.keras.layers.Dropout(dropout2))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Dense(node2, \n",
    "                                        activation = 'relu', kernel_initializer=initializer))\n",
    "        model.add(tf.keras.layers.Dropout(dropout2))\n",
    "\n",
    "        model.add(tf.keras.layers.Dense(node3, \n",
    "                                        activation = 'relu', kernel_initializer=initializer))\n",
    "        model.add(tf.keras.layers.Dropout(dropout3))\n",
    "    model.add(tf.keras.layers.Dense(2, activation = 'softmax', kernel_initializer=initializer))\n",
    "    \n",
    "    model.compile(loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "                      optimizer = tf.keras.optimizers.Adam(learning_rate = lr),\n",
    "                      metrics = F1Score(num_classes = 2))\n",
    "    \n",
    "    # Y값을 원핫 인코딩하여 구성\n",
    "    y_train_one_hot = pd.get_dummies(y_train.is_converted).values.reshape(-1,2)\n",
    "    y_test_one_hot = pd.get_dummies(y_test.is_converted).values.reshape(-1,2)\n",
    "    y_val_one_hot =pd.get_dummies( y_val.is_converted).values.reshape(-1,2)\n",
    "    \n",
    "    history = model.fit(np.array(x_train).reshape(-1,x_train.shape[1]), y_train_one_hot,\n",
    "                             epochs = 30,\n",
    "                             validation_data = [np.array(x_val).reshape(-1,x_train.shape[1]), y_val_one_hot],\n",
    "                            verbose = 2)\n",
    "    error = model.evaluate(x_test, y_test_one_hot)\n",
    "    \n",
    "    if num_layers == \"two\":\n",
    "        params = [patience, min_delta, node1, dropout1,node2, dropout2, lr]\n",
    "    else:\n",
    "        params = [patience, min_delta, node1, dropout1,node2, dropout2,node3, dropout3, lr]\n",
    "        \n",
    "    Parameter_loss.loc[len(Parameter_loss)] = [params,error[1][1]]\n",
    "    pred = model.predict(x_test)\n",
    "    f1 = f1_score(pred[:,1] > 0.5, y_test.is_converted.values)\n",
    "    return f1\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "start = time.time()\n",
    "study.optimize(objective, n_trials=50)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = study.best_trial.params\n",
    "# best 파라미터로 확인\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_f1_score',\n",
    "                                                      mode = 'min',\n",
    "                                                      patience = best[\"patience\"],\n",
    "                                                      min_delta = best[\"min_delta\"])\n",
    "\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "initializer = tf.keras.initializers.HeNormal(seed = 42)\n",
    "    \n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(best[\"node1\"],activation = 'relu', kernel_initializer=initializer))\n",
    "model.add(tf.keras.layers.Dropout(best[\"dropout1\"]))\n",
    "model.add(tf.keras.layers.Dense(best[\"node2\"], activation = 'relu', kernel_initializer=initializer))\n",
    "model.add(tf.keras.layers.Dropout(best[\"dropout2\"]))\n",
    "model.add(tf.keras.layers.Dense(best[\"node3\"], activation = 'relu', kernel_initializer=initializer))\n",
    "model.add(tf.keras.layers.Dropout(best[\"dropout3\"]))\n",
    "model.add(tf.keras.layers.Dense(2, activation = 'softmax', kernel_initializer=initializer))\n",
    "    \n",
    "model.compile(loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "                      optimizer = tf.keras.optimizers.Adam(learning_rate = best[\"lr\"]),\n",
    "                      metrics = F1Score(num_classes = 2))\n",
    "    \n",
    "y_train_one_hot = pd.get_dummies(y_train.is_converted).values.reshape(-1,2)\n",
    "y_test_one_hot = pd.get_dummies(y_test.is_converted).values.reshape(-1,2)\n",
    "y_val_one_hot =pd.get_dummies( y_val.is_converted).values.reshape(-1,2)\n",
    "    \n",
    "history = model.fit(np.array(x_train).reshape(-1,x_train.shape[1]), y_train_one_hot,\n",
    "                             epochs = 30,\n",
    "                             validation_data = [np.array(x_val).reshape(-1,x_train.shape[1]), y_val_one_hot],\n",
    "                            verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_test.values)\n",
    "f1_score(preds[:,1] > 0.5, y_test.values.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"complete_train_duplication.csv\")\n",
    "submission = pd.read_csv(\"complete_submission_duplication.csv\")\n",
    "\n",
    "converted_mean = sum(train.is_converted)/len(train)\n",
    "\n",
    "original_train = pd.read_csv('conversion_train_etc_duplicate.csv')\n",
    "original_sub = pd.read_csv('conversion_sub_etc_duplicate.csv')\n",
    "\n",
    "########################### mean_customer_idx 컬럼 생성 ###########################################\n",
    "original_train['customer_idx_cnt'] = original_train.groupby(['customer_idx'])['customer_idx'].transform('count')\n",
    "\n",
    "#빈도 수 (customer_idx_cnt)가 5 이하인 데이터는 customer_idx를 etc로 통일\n",
    "n = 5\n",
    "for i in range(1,n+1):\n",
    "    original_train.loc[original_train['customer_idx_cnt'] == i,'customer_idx'] = 'etc'\n",
    "\n",
    "# customer_idx 별 평균 영업 성공률 계산한 mean_customer_idx 추가\n",
    "original_train['mean_customer_idx'] = original_train.groupby('customer_idx')['is_converted'].transform('mean')\n",
    "\n",
    "original_train.mean_customer_idx = [converted_mean if original_train.customer_idx.iloc[i] == \"etc\" else original_train.mean_customer_idx.iloc[i] for i in range(len(original_train))]\n",
    "mean_customer_idx_unique = original_train.drop_duplicates(subset=['customer_idx'])[['customer_idx', 'mean_customer_idx']]\n",
    "\n",
    "original_sub = original_sub.merge(mean_customer_idx_unique, on='customer_idx', how='left')\n",
    "original_sub['mean_customer_idx'].fillna(original_train[original_train['customer_idx']=='etc']['mean_customer_idx'].iloc[0],inplace= True)\n",
    "\n",
    "##################### mean_lead_owner 컬럼 생성 ###################################\n",
    "original_train['lead_owner_cnt'] = original_train.groupby(['lead_owner'])['is_converted'].transform('count')\n",
    "\n",
    "#빈도 수 (lead_owner_cnt)가 5 이하인 데이터는 lead_owner를 etc로 통일\n",
    "n = 5\n",
    "for i in range(1,n+1):\n",
    "    original_train.loc[original_train['lead_owner_cnt'] == i,'lead_owner'] = 'etc'\n",
    "\n",
    "# lead_owner 별 평균 영업 성공률 계산한 mean_lead_owner 추가\n",
    "original_train['mean_lead_owner'] = original_train.groupby('lead_owner')['is_converted'].transform('mean')\n",
    "original_train.mean_lead_owner = [converted_mean if original_train.lead_owner.iloc[i] == \"etc\" else original_train.mean_lead_owner.iloc[i] for i in range(len(original_train))]\n",
    "mean_customer_idx_unique = original_train.drop_duplicates(subset=['lead_owner'])[['lead_owner', 'mean_lead_owner']]\n",
    "\n",
    "original_sub = original_sub.merge(mean_customer_idx_unique,on='lead_owner',how='left')\n",
    "original_sub['mean_lead_owner'].fillna(original_train[original_train['lead_owner']=='etc']['mean_lead_owner'].iloc[0],inplace= True)\n",
    "\n",
    "##################### corporate_converted_ratio 컬럼 생성 ###################################\n",
    "table = original_train.value_counts([\"response_corporate\",\"is_converted\"]).reset_index().pivot(index = \"is_converted\", columns = \"response_corporate\", values = 0).T\n",
    "table[\"Total\"] = table.sum(axis = 1)\n",
    "table[\"True_per\"] = table[True]/table[\"Total\"]\n",
    "table = table.fillna(0)\n",
    "table = table.reset_index()[[\"response_corporate\", \"True_per\"]].set_index(\"response_corporate\").T.to_dict()\n",
    "original_train['corporate_converted_ratio'] = [table[i][\"True_per\"] if i in table.keys() else converted_mean for i in original_train.response_corporate]\n",
    "original_sub['corporate_converted_ratio'] = [table[i][\"True_per\"] if i in table.keys() else converted_mean for i in original_sub.response_corporate]\n",
    "\n",
    "\n",
    "train.mean_customer_idx = original_train.mean_customer_idx\n",
    "submission.mean_customer_idx = original_sub.mean_customer_idx\n",
    "\n",
    "train.mean_lead_owner = original_train.mean_lead_owner\n",
    "submission.mean_lead_owner = original_sub.mean_lead_owner\n",
    "\n",
    "train.corporate_converted_ratio = original_train.corporate_converted_ratio\n",
    "submission.corporate_converted_ratio = original_sub.corporate_converted_ratio\n",
    "\n",
    "### 학습\n",
    "train = train.drop(['lead_desc_length','expected_timeline','ver_cus',\"bant_submit\",\"historical_existing_cnt\",\"id_strategic_ver\", \"it_strategic_ver\",\"response_corporate\",\"customer_country\", \"customer_job\",\"customer_position\",'ver_win_rate_x','ver_win_ratio_per_bu', \"inquiry_type_isna\"], axis =1)\n",
    "submission = submission.drop(['lead_desc_length','expected_timeline','ver_cus',\"bant_submit\",\"historical_existing_cnt\",\"id_strategic_ver\", \"it_strategic_ver\",\"response_corporate\",\"customer_country\", \"customer_job\",\"customer_position\",'ver_win_rate_x','ver_win_ratio_per_bu', \"inquiry_type_isna\"], axis =1)\n",
    "\n",
    "\n",
    "# 범주형, 수치형 변수로 나눔\n",
    "target = [\"is_converted\"]\n",
    "categorical = [i for i in train.columns if train[i].dtype == \"O\"]\n",
    "numeric = train.columns.drop(categorical + target).tolist()\n",
    "\n",
    "train[\"구분\"] = \"train\"\n",
    "submission[\"구분\"] = \"sub\"\n",
    "\n",
    "new = pd.concat([train, submission]).reset_index(drop = True)\n",
    "\n",
    "# 인코딩\n",
    "c = []\n",
    "for i in categorical:\n",
    "    encoder = OneHotEncoder(drop='first')\n",
    "    encoder.fit(new[[i]])\n",
    "    new_df = encoder.transform(new[[i]])\n",
    "    \n",
    "    new_df = pd.DataFrame(new_df.toarray(), columns = encoder.get_feature_names_out([i]))\n",
    "    new = new.drop([i], axis = 1)\n",
    "    new = pd.concat([new, new_df], axis = 1)\n",
    "    c += encoder.get_feature_names_out([i]).tolist()\n",
    "\n",
    "categorical = c\n",
    "train = new[new.구분 == \"train\"].drop([\"구분\"], axis = 1).reset_index(drop = True)\n",
    "submission = new[new.구분 == \"sub\"].drop([\"구분\"], axis = 1).reset_index(drop = True)\n",
    "\n",
    "train = train.drop([\"id\"], axis = 1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "t = scaler.fit_transform(train[categorical+numeric])\n",
    "train[categorical+numeric] = pd.DataFrame(t, columns = categorical+numeric)\n",
    "\n",
    "t = scaler.transform(submission[categorical+numeric])\n",
    "submission[categorical+numeric] = pd.DataFrame(t, columns = categorical+numeric)\n",
    "\n",
    "X = train[train.columns.drop(target)]\n",
    "y = train[target]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, shuffle = True)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "best = {'patience': 2,\n",
    " 'min_delta': 0.004,\n",
    " 'node1': 16,\n",
    " 'dropout1': 0.6773681174114016,\n",
    " 'node2': 16,\n",
    " 'dropout2': 0.848566979415723,\n",
    " 'num_layers': 'three',\n",
    " 'node3': 16,\n",
    " 'dropout3': 0.6351770236847896,\n",
    " 'lr': 0.005453416800200536}\n",
    "\n",
    "\n",
    "tf.random.set_seed(89)\n",
    "initializer = tf.keras.initializers.HeNormal(seed = 89)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_f1_score',\n",
    "                                                      mode = 'min',\n",
    "                                                      patience = best[\"patience\"],\n",
    "                                                      min_delta = best[\"min_delta\"])\n",
    "\n",
    "    \n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(best[\"node1\"],activation = 'relu', kernel_initializer=initializer))\n",
    "model.add(tf.keras.layers.Dropout(best[\"dropout1\"]))\n",
    "model.add(tf.keras.layers.Dense(best[\"node2\"], activation = 'relu', kernel_initializer=initializer))\n",
    "model.add(tf.keras.layers.Dropout(best[\"dropout2\"]))\n",
    "model.add(tf.keras.layers.Dense(best[\"node3\"], activation = 'relu', kernel_initializer=initializer))\n",
    "model.add(tf.keras.layers.Dropout(best[\"dropout3\"]))\n",
    "model.add(tf.keras.layers.Dense(2, activation = 'softmax', kernel_initializer=initializer))\n",
    "    \n",
    "model.compile(loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "                      optimizer = tf.keras.optimizers.Adam(learning_rate = best[\"lr\"]),\n",
    "                      metrics = F1Score(num_classes = 2))\n",
    "    \n",
    "y_train_one_hot = pd.get_dummies(y_train.is_converted).values.reshape(-1,2)\n",
    "y_test_one_hot = pd.get_dummies(y_test.is_converted).values.reshape(-1,2)\n",
    "    \n",
    "history = model.fit(np.array(x_train).reshape(-1,x_train.shape[1]), y_train_one_hot,\n",
    "                             epochs = 20,\n",
    "                             validation_data = [np.array(x_test).reshape(-1,x_train.shape[1]), y_test_one_hot],\n",
    "                            verbose = 2)\n",
    "preds = model.predict(submission.drop([\"id\", \"is_converted\"], axis = 1).values)\n",
    "preds = preds[:,1] > 0.5\n",
    "\n",
    "submission = pd.read_csv(\"submission.csv\")\n",
    "submission['is_converted'] = preds\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "submission.to_csv(\"submission_mlp.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
